{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain langchain-chroma langchain-openai chroma langchainhub"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kf0WMTIt7_XW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "245d365e-8a31-4675-894c-e3e2ec895790",
        "id": "MolCS8DpD4rY"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Standard Library Imports\n",
        "import os\n",
        "import requests\n",
        "import getpass\n",
        "\n",
        "# BeautifulSoup for HTML Parsing\n",
        "import bs4\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# NLTK for Natural Language Processing\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# langchain for Language Model Operations\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# NLTK Setup\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1adf0e8-aa8a-4e0a-e05c-80073dfdf8d0",
        "id": "BW8sy8AND4rZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "# Get API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the WebBaseLoader class\n",
        "class WebBaseLoader:\n",
        "    # Constructor to initialize the WebBaseLoader object with web_paths and bs_kwargs\n",
        "    def __init__(self, web_paths, bs_kwargs):\n",
        "        self.web_paths = web_paths  # Stores a list of URLs to be processed\n",
        "        self.bs_kwargs = bs_kwargs  # Stores additional arguments for BeautifulSoup\n",
        "\n",
        "    # Method to load data from each web path and parse the HTML content\n",
        "    def load(self):\n",
        "        results = {}  # Dictionary to store the results of web scraping\n",
        "        for url in self.web_paths:  # Iterating over each URL in the web_paths list\n",
        "            try:\n",
        "                response = requests.get(url)  # Sending a GET request to the URL\n",
        "                if response.status_code == 200:  # Checking if the request was successful\n",
        "                    # Parsing the HTML content with BeautifulSoup using the provided arguments\n",
        "                    soup = BeautifulSoup(response.text, 'html.parser', **self.bs_kwargs)\n",
        "                    results[url] = soup.get_text()  # Extracting text from the parsed HTML and storing it in the results dictionary\n",
        "                else:\n",
        "                    results[url] = None  # Storing None if the response was unsuccessful\n",
        "            except requests.RequestException as e:  # Handling exceptions that may occur during the GET request\n",
        "                results[url] = str(e)  # Storing the exception message as the result for the URL\n",
        "        return results  # Returning the dictionary containing the results of the web scraping"
      ],
      "metadata": {
        "id": "aDa0vwBLD4rZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gOPHTrGwD4rZ"
      },
      "outputs": [],
      "source": [
        "# Function definition to fetch and return text content from specified website URLs using a given set of selector attributes\n",
        "def fetch_website_text(urls, selector_attrs):\n",
        "    # Creating a SoupStrainer object that filters out all unnecessary data except for elements matching the provided attributes\n",
        "    strainer = bs4.SoupStrainer(**selector_attrs)\n",
        "    # Initializing the WebBaseLoader with the URLs and the strainer object to parse only necessary parts of HTML\n",
        "    loader = WebBaseLoader(web_paths=urls, bs_kwargs={\"parse_only\": strainer})\n",
        "    # Calling the 'load' method from the WebBaseLoader instance to fetch and parse the web pages\n",
        "    return loader.load()\n",
        "\n",
        "# List of URLs from which to scrape data\n",
        "urls = [\n",
        "    \"https://www.theguardian.com/technology/2016/may/03/amazon-fresh-food-deliveries-understood-to-start-this-month\",\n",
        "    \"https://www.theguardian.com/media/2016/may/16/bbc-netflix-rival-itv-nbc-universal\",\n",
        "    \"https://www.theguardian.com/technology/2016/apr/28/amazon-most-profitable-quarter-sales-up-costs\",\n",
        "    \"https://www.theguardian.com/technology/2016/apr/26/amazon-kindle-oasis-review-luxury-e-reader\",\n",
        "    \"https://www.theguardian.com/environment/andes-to-the-amazon/2016/may/25/london-stock-exchange-amazon-deforestation\",\n",
        "    \"https://www.theguardian.com/media/2016/may/25/netflix-and-amazon-must-guarantee-20-of-content-is-european\",\n",
        "    \"https://www.theguardian.com/technology/2016/may/26/amazon-echo-virtual-assistant-child-privacy-law\",\n",
        "]\n",
        "# Dictionary specifying the attributes to filter HTML elements using SoupStrainer\n",
        "selector_attrs = {\"class\": \"article-body-commercial-selector\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The function fetch_website_text is now ready to be called with the list of URLs and selector attributes"
      ],
      "metadata": {
        "id": "U066izP1Sy5q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dhkRHILWD4ra"
      },
      "outputs": [],
      "source": [
        "# Initialize the WebBaseLoader with URLs and BeautifulSoup keyword arguments\n",
        "loader = WebBaseLoader(urls, {\"parse_only\": bs4.SoupStrainer(**selector_attrs)})\n",
        "\n",
        "# Load the content from the specified URLs\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DGu7hfy2D4ra"
      },
      "outputs": [],
      "source": [
        "# Definition of the RecursiveCharacterTextSplitter class\n",
        "class RecursiveCharacterTextSplitter:\n",
        "    # Constructor to initialize the RecursiveCharacterTextSplitter object with chunk_size and chunk_overlap\n",
        "    def __init__(self, chunk_size, chunk_overlap):\n",
        "        self.chunk_size = chunk_size  # The number of characters in each text chunk\n",
        "        self.chunk_overlap = chunk_overlap  # The number of characters each chunk overlaps with the next\n",
        "\n",
        "    # Method to split a single text string into smaller chunks based on chunk_size and chunk_overlap\n",
        "    def split_document(self, text):\n",
        "        # Creates a list of text chunks by iterating over the text. The starting index for each chunk is adjusted by the chunk_size minus the chunk_overlap.\n",
        "        return [text[i:i + self.chunk_size] for i in range(0, len(text), self.chunk_size - self.chunk_overlap)]\n",
        "\n",
        "    # Method to split multiple documents, where each document is a string or an object containing a string attribute\n",
        "    def split_documents(self, documents):\n",
        "        splits = []  # List to hold all chunks from all documents\n",
        "        for doc in documents:  # Iterating over each document in the provided list\n",
        "            if isinstance(doc, str):\n",
        "                text = doc  # Directly assigns the document to text if it is a string\n",
        "            else:\n",
        "                text = getattr(doc, 'page_content', '')  # Attempts to fetch 'page_content' from the document object; defaults to empty string if not found\n",
        "            splits.extend(self.split_document(text))  # Adds the chunks from the current document to the splits list\n",
        "        return splits  # Returns the list of all chunks from all documents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an instance of RecursiveCharacterTextSplitter with a chunk size of 1000 characters and an overlap of 200 characters\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "# Splitting a list of documents into smaller, overlapping chunks to maintain context between sections\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Initializing a vector store to enable semantic search capabilities using embeddings from OpenAI\n",
        "vectorstore = Chroma.from_texts(texts=splits, embedding=OpenAIEmbeddings())\n",
        "# Creating a retriever from the vector store for efficient information retrieval\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Retrieving a pre-defined prompt designed for use with language models in a retrieval-augmented generation setup\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "# Initializing a ChatOpenAI instance with the specified model to use for generating responses based on the prompt\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
      ],
      "metadata": {
        "id": "dDJtS21YTF6m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lDuCNeazD4rb"
      },
      "outputs": [],
      "source": [
        "# Function to format strings from a list of documents into a single string\n",
        "def format_strings(documents):\n",
        "    formatted_documents = []  # List to hold formatted documents\n",
        "    for doc in documents:  # Iterate through each document in the input list\n",
        "        if isinstance(doc, str):\n",
        "            formatted_documents.append(doc)  # Add the string directly if the document is a string\n",
        "        elif isinstance(doc, dict):\n",
        "            # If the document is a dictionary, retrieve the value of 'page_content', defaulting to an empty string if not found\n",
        "            formatted_documents.append(doc.get('page_content', ''))\n",
        "        else:\n",
        "            # Append an empty string if the document is neither a string nor a dictionary\n",
        "            formatted_documents.append('')\n",
        "    # Join all formatted documents into a single string, separated by two newlines\n",
        "    return \"\\n\\n\".join(formatted_documents)\n",
        "\n",
        "# Usage of the function to format a list of documents\n",
        "formatted_context = format_strings(docs)\n",
        "\n",
        "# Define a retrieval-augmented generation chain using a context retriever and a question passthrough in a pipeline setup\n",
        "rag_chain = (\n",
        "    # Lambda function to fetch relevant documents based on the context provided by the retriever\n",
        "    {\"context\": lambda x: retriever.get_relevant_documents(x), \"question\": RunnablePassthrough()} |\n",
        "    # Incorporates the prompt, likely intended to direct the language model's response generation\n",
        "    prompt |\n",
        "    # Final component of the chain, a language model from OpenAI configured to generate responses\n",
        "    llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# References and Summaries (manually sourced and created) for BLEU score calculation\n",
        "references = {\n",
        "    \"https://www.theguardian.com/technology/2016/may/03/amazon-fresh-food-deliveries-understood-to-start-this-month\": \"Amazon is believed to be planning to start delivering fresh food in the UK this month.\",\n",
        "    \"https://www.theguardian.com/media/2016/may/16/bbc-netflix-rival-itv-nbc-universal\": \"BBC and ITV partner with NBC Universal to rival Netflix.\",\n",
        "    \"https://www.theguardian.com/technology/2016/apr/28/amazon-most-profitable-quarter-sales-up-costs\": \"Amazon reports its most profitable quarter with increased sales.\",\n",
        "    \"https://www.theguardian.com/technology/2016/apr/26/amazon-kindle-oasis-review-luxury-e-reader\": \"Review of Amazon Kindle Oasis, a luxury e-reader.\",\n",
        "    \"https://www.theguardian.com/environment/andes-to-the-amazon/2016/may/25/london-stock-exchange-amazon-deforestation\": \"London Stock Exchange faces scrutiny over Amazon deforestation.\",\n",
        "    \"https://www.theguardian.com/media/2016/may/25/netflix-and-amazon-must-guarantee-20-of-content-is-european\": \"Netflix and Amazon must ensure 20% of their content is European.\",\n",
        "    \"https://www.theguardian.com/technology/2016/may/26/amazon-echo-virtual-assistant-child-privacy-law\": \"Amazon Echo virtual assistant faces child privacy law issues.\"\n",
        "}"
      ],
      "metadata": {
        "id": "QUsXcFjw9Xpq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the BLEU score between a hypothesis and a reference text\n",
        "def calculate_bleu(hypothesis, reference):\n",
        "    # Tokenizing the reference text into words\n",
        "    reference_tokens = word_tokenize(reference)\n",
        "    # Tokenizing the hypothesis text into words\n",
        "    hypothesis_tokens = word_tokenize(hypothesis)\n",
        "    # Creating a smoothing function to handle cases where precision is 0 (common in short texts or specific domains)\n",
        "    smoothing_function = SmoothingFunction().method1\n",
        "    # Calculating the BLEU score using the sentence-level BLEU function, suitable for comparing a pair of sentences\n",
        "    return sentence_bleu([reference_tokens], hypothesis_tokens, smoothing_function=smoothing_function)"
      ],
      "metadata": {
        "id": "NkE33vV29ZtF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BEVHtBsmMcF",
        "outputId": "f23d6d7a-f9e1-41ce-c344-b29f421d9af5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question about URL: https://www.theguardian.com/technology/2016/may/03/amazon-fresh-food-deliveries-understood-to-start-this-month\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Response: The article from The Guardian discusses the launch of Amazon Fresh food deliveries expected to start in the same month. This new service will allow customers to order fresh groceries online. Amazon aims to compete with other food delivery services by offering a convenient and efficient option for customers.\n",
            "BLEU Score: 0.02465289405820389\n",
            "\n",
            "Question about URL: https://www.theguardian.com/media/2016/may/16/bbc-netflix-rival-itv-nbc-universal\n",
            "Generated Response: \"The article discusses the BBC\n",
            "BLEU Score: 0.01774239756616722\n",
            "\n",
            "Question about URL: https://www.theguardian.com/technology/2016/apr/28/amazon-most-profitable-quarter-sales-up-costs\n",
            "Generated Response: \"The article discusses Amazon\n",
            "BLEU Score: 0.01976560930094397\n",
            "\n",
            "Question about URL: https://www.theguardian.com/technology/2016/apr/26/amazon-kindle-oasis-review-luxury-e-reader\n",
            "Generated Response: \"The article discusses the review of the Amazon Kindle Oasis, highlighting its luxury features and design. It provides an in-depth analysis of the e-reader\n",
            "BLEU Score: 0.10332090908268508\n",
            "\n",
            "Question about URL: https://www.theguardian.com/environment/andes-to-the-amazon/2016/may/25/london-stock-exchange-amazon-deforestation\n",
            "Generated Response: The article discusses how a coalition of environmentalists and indigenous leaders are pressuring the London Stock Exchange to stop trading in companies that are linked to deforestation in the Amazon. The activists are calling for financial institutions to take responsibility for their investments and prevent further destruction of the rainforest. The campaign aims to hold companies and investors accountable for their role in contributing to deforestation in the Amazon.\n",
            "BLEU Score: 0.014847755848337078\n",
            "\n",
            "Question about URL: https://www.theguardian.com/media/2016/may/25/netflix-and-amazon-must-guarantee-20-of-content-is-european\n",
            "Generated Response: The article discusses the requirement for Netflix and Amazon to ensure that at least 20% of their content is European. This initiative aims to promote European cultural diversity and support local content creation. It highlights the need for streaming services to contribute to the European media landscape.\n",
            "BLEU Score: 0.1584450133726893\n",
            "\n",
            "Question about URL: https://www.theguardian.com/technology/2016/may/26/amazon-echo-virtual-assistant-child-privacy-law\n",
            "Generated Response: \"The article discusses concerns regarding Amazon Echo and child privacy laws. It focuses on potential issues with data collection and privacy when children interact with virtual assistants like Amazon Echo. The article highlights the need for clearer regulations to protect children\n",
            "BLEU Score: 0.014397710577076568\n",
            "\n",
            "Average BLEU Score: 0.050453184258014726\n"
          ]
        }
      ],
      "source": [
        "# Function to interact with a retrieval-augmented generation system and evaluate BLEU scores\n",
        "def main():\n",
        "    bleu_scores = []  # List to store the BLEU scores for each response\n",
        "\n",
        "    # Looping over a dictionary 'references' that contains URLs and their corresponding reference summaries\n",
        "    for url, ref_summary in references.items():\n",
        "        print(f\"Question about URL: {url}\")  # Printing the URL being processed\n",
        "        # Invoking the RAG chain to generate a summary for the content of the URL\n",
        "        response = rag_chain.invoke(f\"Summarize the main content of the article from URL: {url}\")\n",
        "        response_str = str(response)  # Converting the response to a string\n",
        "\n",
        "        # Extracting the content from the response string based on predefined format\n",
        "        content_start = response_str.find(\"content='\") + len(\"content='\")\n",
        "        content_end = response_str.find(\"'\", content_start)\n",
        "        content = response_str[content_start:content_end]\n",
        "\n",
        "        print(\"Generated Response:\", content)  # Printing the generated content\n",
        "        # Calculating the BLEU score for the generated content against the reference summary\n",
        "        bleu_score = calculate_bleu(content, ref_summary)\n",
        "        bleu_scores.append(bleu_score)  # Appending the BLEU score to the list\n",
        "        print(f\"BLEU Score: {bleu_score}\\n\")  # Printing the BLEU score\n",
        "\n",
        "    # Calculating the average BLEU score from all scores computed\n",
        "    average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "    print(f\"Average BLEU Score: {average_bleu}\")  # Printing the average BLEU score\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Main function to interactively query the system and display responses\n",
        "def main():\n",
        "    while True:\n",
        "        question = input(\"Ask a question about Amazon between April and June 2016 (or type 'exit' to quit): \")\n",
        "        if question.lower() == 'exit':\n",
        "            break\n",
        "        response = rag_chain.invoke(question)\n",
        "        response_str = str(response)\n",
        "        content_start = response_str.find(\"content='\") + len(\"content='\")\n",
        "        content_end = response_str.find(\"'\", content_start) + 1\n",
        "        content = response_str[content_start:content_end]\n",
        "        print(\"Response:\", content)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "7i58TSSiLpTK",
        "outputId": "69d1e1ff-94a9-4815-b15c-548d2d3bd823"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a question about Amazon between April and June 2016 (or type 'exit' to quit): Summarise the news\n",
            "Response: The news covers topics such as the competition between BBC and Netflix, concerns about deforestation linked to the London Stock Exchange and Amazon, and the call for Netflix and Amazon to ensure a percentage of their content is European. Additionally, there is a review of the Amazon Kindle Oasis as a luxury e-reader.'\n",
            "Ask a question about Amazon between April and June 2016 (or type 'exit' to quit): What are the concerns?\n",
            "Response: \"The concerns raised in the retrieved context include child privacy laws in relation to Amazon Echo, the impact of Amazon Fresh food deliveries starting, the deforestation issues linked to Amazon, and Amazon'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-5bee14c45c07>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-5bee14c45c07>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ask a question about Amazon between April and June 2016 (or type 'exit' to quit): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}