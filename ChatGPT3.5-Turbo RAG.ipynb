{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D6c92KhTV6M2"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import requests\n",
        "from langchain_openai import ChatOpenAI\n",
        "import bs4\n",
        "from bs4 import BeautifulSoup\n",
        "import getpass\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypLjHMfGWL3k",
        "outputId": "4376903d-670b-4ac4-d22c-2cbcf1f776c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "# Get API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "knXqYsNzUXmt"
      },
      "outputs": [],
      "source": [
        "# Define a class for loading web pages based on URLs\n",
        "class WebBaseLoader:\n",
        "    def __init__(self, web_paths, bs_kwargs):\n",
        "        # Initialize with a list of URLs and BeautifulSoup configurations\n",
        "        self.web_paths = web_paths\n",
        "        self.bs_kwargs = bs_kwargs\n",
        "\n",
        "    def load(self):\n",
        "        # Method to fetch content from each URL\n",
        "        results = {}\n",
        "        for url in self.web_paths:\n",
        "            try:\n",
        "                # Perform a HTTP GET request to the URL\n",
        "                response = requests.get(url)\n",
        "                # If the response is successful, process the text with BeautifulSoup\n",
        "                if response.status_code == 200:\n",
        "                    soup = BeautifulSoup(response.text, 'html.parser', parse_only=self.bs_kwargs['parse_only'])\n",
        "                    results[url] = soup.get_text()\n",
        "                else:\n",
        "                    # Store None if the request was unsuccessful\n",
        "                    results[url] = None\n",
        "            except requests.RequestException as e:\n",
        "                # Store the exception message if an error occurred during the request\n",
        "                results[url] = str(e)\n",
        "                results[url] = str(e)\n",
        "        return results\n",
        "\n",
        "    def save_to_document(self, filename='document.txt'):\n",
        "        # Method to save the fetched content to a file\n",
        "        results = self.load()\n",
        "        with open(filename, 'w', encoding='utf-8') as file:\n",
        "            for url, text in results.items():\n",
        "                # Write each URL and its content to the file\n",
        "                file.write(f'# URL: {url}\\n{text}\\n\\n')\n",
        "\n",
        "# Function to initialize and use the WebBaseLoader\n",
        "def fetch_website_text(urls, selector_attrs):\n",
        "    # Define the HTML elements to parse\n",
        "    strainer = bs4.SoupStrainer(**selector_attrs)\n",
        "    # Create an instance of the loader with the specified URLs and parsing filter\n",
        "    loader = WebBaseLoader(\n",
        "        web_paths=urls,\n",
        "        bs_kwargs={\"parse_only\": strainer}\n",
        "    )\n",
        "    # Fetch the data\n",
        "    return loader.load()\n",
        "\n",
        "# URLs to be fetched\n",
        "urls = [\n",
        "    \"https://www.theguardian.com/technology/2016/may/03/amazon-fresh-food-deliveries-understood-to-start-this-month\",\n",
        "    \"https://www.theguardian.com/media/2016/may/16/bbc-netflix-rival-itv-nbc-universal\",\n",
        "    \"https://www.theguardian.com/technology/2016/apr/28/amazon-most-profitable-quarter-sales-up-costs\",\n",
        "    \"https://www.theguardian.com/technology/2016/apr/26/amazon-kindle-oasis-review-luxury-e-reader\",\n",
        "    \"https://www.theguardian.com/environment/andes-to-the-amazon/2016/may/25/london-stock-exchange-amazon-deforestation\",\n",
        "    \"https://www.theguardian.com/media/2016/may/25/netflix-and-amazon-must-guarantee-20-of-content-is-european\",\n",
        "    \"https://www.theguardian.com/technology/2016/may/26/amazon-echo-virtual-assistant-child-privacy-law\",\n",
        "]\n",
        "# HTML class attributes used for filtering content\n",
        "selector_attrs1 = {\"class\": \"dcr-1qg0p6f\"}\n",
        "selector_attrs2 = {\"class\": \"dcr-1qg0p6f\"}\n",
        "selector_attrs3 = {\"class\": \"dcr-1qg0p6f\"}\n",
        "selector_attrs4 = {\"class\": \"dcr-1qg0p6f\"}\n",
        "selector_attrs5 = {\"class\": \"dcr-1qg0p6f\"}\n",
        "selector_attrs6 = {\"class\": \"dcr-1qg0p6f\"}\n",
        "selector_attrs7 = {\"class\": \"dcr-1qg0p6f\"}\n",
        "\n",
        "# Initialize the loader with the URLs and the specific attribute filter\n",
        "loader = WebBaseLoader(urls, {\"parse_only\": bs4.SoupStrainer(**selector_attrs1)})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rQ6D98x3VHKO"
      },
      "outputs": [],
      "source": [
        "# After fetching documents using a previously defined loader (WebBaseLoader)\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "t-I2WgxmVq5C"
      },
      "outputs": [],
      "source": [
        "# Define a class that handles the splitting of text into smaller chunks with a defined size and overlap\n",
        "class RecursiveCharacterTextSplitter:\n",
        "    def __init__(self, chunk_size, chunk_overlap):\n",
        "        # Constructor to initialize with chunk size and the overlap between consecutive chunks\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "\n",
        "    def split_document(self, text):\n",
        "        \"\"\"Method to split a single document into overlapping chunks.\n",
        "        It generates a list of substrings, each of a specified 'chunk_size', with 'chunk_overlap' characters overlapping between adjacent chunks.\n",
        "        \"\"\"\n",
        "        return [text[i:i + self.chunk_size] for i in range(0, len(text), self.chunk_size - self.chunk_overlap)]\n",
        "\n",
        "    def split_documents(self, documents):\n",
        "        \"\"\"Method to split multiple documents which may be either strings or objects with a 'page_content' attribute. \n",
        "        This method loops through each document, checks its type, extracts text, and applies 'split_document'.\n",
        "        \"\"\"\n",
        "        splits = []\n",
        "        for doc in documents:\n",
        "            # Check if 'doc' is a string or has attribute 'page_content'\n",
        "            if isinstance(doc, str):\n",
        "                text = doc  # Treat doc as plain text\n",
        "            else:\n",
        "                text = getattr(doc, 'page_content', '')\n",
        "            splits.extend(self.split_document(text))\n",
        "        return splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate the text splitter and split loaded documents into manageable chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FNyfyFNqVzvg"
      },
      "outputs": [],
      "source": [
        "# Initialize a vector store with the split texts using embeddings from OpenAI to create semantic search capabilities\n",
        "vectorstore = Chroma.from_texts(texts=splits, embedding=OpenAIEmbeddings())\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jH1jGCZqVztU"
      },
      "outputs": [],
      "source": [
        "# Load a pre-defined prompt from a hub, intended for use with language models in a retrieval-augmented generation setup\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9OqG1QGDVzrA"
      },
      "outputs": [],
      "source": [
        "# Function to format documents into a uniform structure for processing.\n",
        "def format_strings(documents):\n",
        "    \"\"\" Formats a list of documents by either extracting 'page_content' from dictionaries or using string directly. \n",
        "    Returns a single string with documents separated by two newlines.\n",
        "    \"\"\"\n",
        "    formatted_documents = []\n",
        "    for doc in documents:\n",
        "        if isinstance(doc, str):\n",
        "            formatted_documents.append(doc)\n",
        "        elif isinstance(doc, dict):\n",
        "            # Check and extract 'page_content'\n",
        "            formatted_documents.append(doc.get('page_content', ''))\n",
        "        else:\n",
        "            # Fallback or default string if the document type is not supported\n",
        "            formatted_documents.append('')\n",
        "    return \"\\n\\n\".join(formatted_documents)\n",
        "\n",
        "# Format the documents for use with the response chain\n",
        "formatted_context = format_strings(docs)\n",
        "\n",
        "# Define a retrieval-augmented generation (RAG) chain that combines context retrieval with language model generation\n",
        "rag_chain = (\n",
        "    {\"context\": lambda x: retriever, \"question\": RunnablePassthrough()} | prompt | llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BEVHtBsmMcF",
        "outputId": "84a6ee23-c2de-49cb-9857-ee15fec87c6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask a question about Amazon between April and June 2016 (or type 'exit' to quit): What are the news about Amazon between April and June 2016?\n",
            "Response: \"Amazon reported its most profitable quarter in April 2016 with increased sales and lower costs. In May 2016, Amazon was understood to start fresh food deliveries. Additionally, in the same month, there were concerns about Amazon'\n",
            "Ask a question about Amazon between April and June 2016 (or type 'exit' to quit): Tell me more about Amazon profitable quarter\n",
            "Response: \"Amazon reported its most profitable quarter in April 2016, with sales up and costs down. The increase in profits was attributed to strong demand for Amazon Web Services and growth in its Prime program. This success came despite concerns about the impact of Amazon'\n",
            "Ask a question about Amazon between April and June 2016 (or type 'exit' to quit): Why is the most profitable quarter?\n",
            "Response: The most profitable quarter is typically the one where sales are high and costs are controlled. Amazon experienced its most profitable quarter with increased sales and effective cost management strategies. The details of why a specific quarter is the most profitable can vary based on company performance and market conditions.'\n",
            "Ask a question about Amazon between April and June 2016 (or type 'exit' to quit): Tell me more about Amazon's new launch about fresh food deliveries\n",
            "Response: \"Amazon'\n",
            "Ask a question about Amazon between April and June 2016 (or type 'exit' to quit): Tell me more about Amazon about fresh food deliveries\n",
            "Response: Amazon Fresh is a service that offers fresh food deliveries. It was reported to start in May 2016. The service allows customers to order groceries online and have them delivered to their homes.'\n"
          ]
        }
      ],
      "source": [
        "# Main function to interactively query the system and display responses\n",
        "def main():\n",
        "    while True:\n",
        "        question = input(\"Ask a question about Amazon between April and June 2016 (or type 'exit' to quit): \")\n",
        "        if question.lower() == 'exit':\n",
        "            break\n",
        "        response = rag_chain.invoke(question)\n",
        "        response_str = str(response)\n",
        "        content_start = response_str.find(\"content='\") + len(\"content='\")\n",
        "        content_end = response_str.find(\"'\", content_start) + 1\n",
        "        content = response_str[content_start:content_end]\n",
        "        print(\"Response:\", content)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
